{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer \n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Định nghĩa các biến"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "white_space_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ID_OF_DOC_FOR_QUERY = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = \"s\"\n",
    "# if word in stop_words:\n",
    "#     print(\"true\")\n",
    "# else:\n",
    "#     print(\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các hàm thành phần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getFilePathList(path):\n",
    "    list_file_path = list()\n",
    "    \n",
    "    for file_name in os.listdir(path):\n",
    "        full_file_path = path + \"\\\\\" + file_name\n",
    "        \n",
    "        if os.path.isfile(full_file_path):\n",
    "            list_file_path.append(full_file_path)\n",
    "            \n",
    "    return list_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indexingDocument(list_file_path):\n",
    "    id_path_of_files = dict()\n",
    "    \n",
    "    for index, file_path in enumerate(list_file_path):\n",
    "        id_path_of_files[index] = file_path\n",
    "    \n",
    "    return id_path_of_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessString(string_data, mode_of_preprocessing):\n",
    "    # Lower the text\n",
    "    preprocess_data = string_data.lower()\n",
    "    \n",
    "    # Remove Unicode characters\n",
    "    preprocess_data = preprocess_data.encode('ascii', 'ignore').decode()\n",
    "    \n",
    "    # One letter in a word should not be present more than twice in continuation, ex: \"I misssss youuu\" -> \"I miss youu\"\n",
    "    preprocess_data = ''.join(''.join(s)[:2] for _, s in itertools.groupby(preprocess_data)) \n",
    "    \n",
    "    # Remove punctuations, each punctuation = space, ex: \"\"information @#$retrieval\" -> \"information    retrieval\"\n",
    "    preprocess_data = re.sub('[%s]' % re.escape(string.punctuation), ' ', preprocess_data)   \n",
    "        \n",
    "    # Tokenize word by white space\n",
    "    preprocess_data = white_space_tokenizer.tokenize(preprocess_data)\n",
    "    \n",
    "############################################################################################   \n",
    "    \n",
    "    \n",
    "    # Remove stop words\n",
    "#     preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        \n",
    "    # Stem word\n",
    "#     preprocess_data = [porter_stemmer.stem(word) for word in preprocess_data]\n",
    "  \n",
    "    # Lemmatize word\n",
    "#     preprocess_data = [wordnet_lemmatizer.lemmatize(word) for word in preprocess_data]\n",
    "\n",
    "    if mode_of_preprocessing == 'stopWord-lemmatize':\n",
    "        # Remove stop words\n",
    "        preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        # Lemmatize word\n",
    "        preprocess_data = [wordnet_lemmatizer.lemmatize(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'stopWord-stem':\n",
    "        # Remove stop words\n",
    "        preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        # Stem word\n",
    "        preprocess_data = [porter_stemmer.stem(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'stopWord-noStem':\n",
    "        # Remove stop words\n",
    "        preprocess_data = [word for word in preprocess_data if word not in stop_words]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'noStopWord-lemmatize':\n",
    "        # Lemmatize word\n",
    "        preprocess_data = [wordnet_lemmatizer.lemmatize(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'noStopWord-stem':\n",
    "        # Stem word\n",
    "        preprocess_data = [porter_stemmer.stem(word) for word in preprocess_data]\n",
    "        \n",
    "    elif mode_of_preprocessing == 'noStopWord-noStem':\n",
    "        pass\n",
    "        \n",
    "\n",
    "############################################################################################\n",
    "        \n",
    "    return preprocess_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'have', 'i', 'done', 'and', 'why', 'who', 's', 'this']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preprocess = \"What have I done? And why? Who's this\"\n",
    "test_preprocess = preprocessString(test_preprocess, 'noStopWord-noStem')\n",
    "test_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['done']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preprocess = \"What have I done? And why? Who's this\"\n",
    "test_preprocess = preprocessString(test_preprocess, 'stopWord-noStem')\n",
    "test_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preprocess = \"shoo-doo-doo ab s, sù,\"\n",
    "# test_preprocess = preprocessString(test_preprocess, 'noStopWord-noStem')\n",
    "# test_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_termID_forQuery(dictionary_of_docs, query, mode_of_preprocessing):\n",
    "    term_id = list()\n",
    "    \n",
    "    query = preprocessString(query, mode_of_preprocessing)\n",
    "    \n",
    "    for term in query:\n",
    "        if term in dictionary_of_docs.keys():\n",
    "            term_id.append([term, ID_OF_DOC_FOR_QUERY])\n",
    "            \n",
    "    # No need to sort term_id\n",
    "#     term_id.sort()\n",
    "    \n",
    "    return term_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_termID_forDocument(id_path_of_files, mode_of_preprocessing):\n",
    "    term_id = list()\n",
    "    \n",
    "    for index, file_path in id_path_of_files.items():\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.readlines()\n",
    "            f.close()\n",
    "            \n",
    "            lyric = str()\n",
    "            for i in range(2, len(content)):\n",
    "                lyric = lyric + ' ' + content[i]           \n",
    "            lyric = preprocessString(lyric, mode_of_preprocessing)\n",
    "            \n",
    "            for term in lyric:\n",
    "                term_id.append([term, index])\n",
    "            \n",
    "    # No need to sort term_id\n",
    "#     term_id.sort()\n",
    "    \n",
    "    return term_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createDictionaryAndVectorDoc(term_id):\n",
    "    dictionary = dict()\n",
    "    vector_docs = dict()\n",
    "        \n",
    "    for term, id_of_doc in term_id:\n",
    "        \n",
    "        if id_of_doc not in vector_docs.keys():\n",
    "            vector_docs[id_of_doc] = {term}\n",
    "            \n",
    "        elif term not in vector_docs[id_of_doc]:\n",
    "            vector_docs[id_of_doc].add(term)\n",
    "        \n",
    "        \n",
    "        # Nếu term chưa có trong dictionary thì thêm term, ndoc, id_tf vào\n",
    "        if term not in dictionary.keys():\n",
    "            dictionary[term] = {'ndoc': 1,\n",
    "                                'id_tf': {id_of_doc: 1}}\n",
    "            \n",
    "        # Nếu term đã có trong dictionary rồi thì sẽ cập nhật các chỉ số ndoc, id_tf nếu thỏa điều kiện\n",
    "        else:\n",
    "            \n",
    "            # Nếu term này đã xuất hiện trong id_tf thì chỉ cập nhật mỗi id_tf\n",
    "            # (tức là cập nhật tần số xuất hiện của term trong document này)\n",
    "            if id_of_doc in dictionary[term]['id_tf'].keys():\n",
    "                dictionary[term]['id_tf'][id_of_doc] += 1\n",
    "                \n",
    "            # Nếu term này chưa xuất hiện trong id_tf thì phải cập nhật thêm ndoc lên 1 đơn vị\n",
    "            # và cập nhật thêm một cặp id_tf mới cho term\n",
    "            else:\n",
    "                dictionary[term]['ndoc'] += 1\n",
    "                dictionary[term]['id_tf'][id_of_doc] = 1\n",
    " \n",
    "\n",
    "    # No need to sort id_tf\n",
    "#     for term in dictionary.keys():\n",
    "#         dictionary[term]['id_tf'] = OrderedDict(sorted(dictionary[term]['id_tf'].items())) \n",
    "            \n",
    "    return [dictionary, vector_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay giá trị tf bằng weight tf: 1 + math.log(tf, 10)\n",
    "\n",
    "def calculateTF(dictionary):\n",
    "    for term in dictionary.keys():        \n",
    "        for doc_id in dictionary[term]['id_tf'].keys():\n",
    "            tf = dictionary[term]['id_tf'][doc_id]\n",
    "            dictionary[term]['id_tf'][doc_id] = 1 + math.log(tf, 10)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay giá trị ndoc bằng IDF = log(number_of_docs / ndoc, 10)\n",
    "\n",
    "def calculateIDF(dictionary, number_of_docs):\n",
    "    for term in dictionary.keys():\n",
    "        ndoc = dictionary[term]['ndoc']\n",
    "        dictionary[term]['ndoc'] = math.log(number_of_docs / ndoc, 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateIDF_forQuery(dictionary_of_docs, dictionary_of_query):\n",
    "    for term in dictionary_of_query.keys():\n",
    "        ndoc = dictionary_of_docs[term]['ndoc']\n",
    "        dictionary_of_query[term]['ndoc'] = ndoc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thay giá trị id_tf bằng tf-idf = tf * idf\n",
    "\n",
    "def calculate_TF_IDF(dictionary):\n",
    "    for term in dictionary.keys():\n",
    "        idf_of_term = dictionary[term]['ndoc']\n",
    "        \n",
    "        for id_of_doc in dictionary[term]['id_tf'].keys():\n",
    "            tf_of_term_in_doc = dictionary[term]['id_tf'][id_of_doc]\n",
    "                      \n",
    "            dictionary[term]['id_tf'][id_of_doc] = tf_of_term_in_doc * idf_of_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector là list\n",
    "def calculateDenominatorOfVector(vector):\n",
    "    denominator = 0\n",
    "    \n",
    "    for value in vector:\n",
    "        denominator += math.pow(value, 2)\n",
    "        \n",
    "    sqrt_denominator = math.sqrt(denominator)\n",
    "    \n",
    "    if sqrt_denominator > 0:\n",
    "        return sqrt_denominator\n",
    "    else:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizeDictionary(dictionary, vector_docs):\n",
    "    \n",
    "    for id_of_doc in vector_docs.keys():\n",
    "        vector_weight_of_document = list()\n",
    "        \n",
    "        for term in vector_docs[id_of_doc]:\n",
    "            tf_idf_of_term = dictionary[term]['id_tf'][id_of_doc]\n",
    "            vector_weight_of_document.append(tf_idf_of_term)\n",
    "            \n",
    "        denominator_of_vector = calculateDenominatorOfVector(vector_weight_of_document)\n",
    "        \n",
    "        for term in vector_docs[id_of_doc]:\n",
    "            tf_idf_of_term = dictionary[term]['id_tf'][id_of_doc]\n",
    "            dictionary[term]['id_tf'][id_of_doc] = tf_idf_of_term / denominator_of_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Union(set_of_doc, set_of_query):    \n",
    "    union_set = set().union(set_of_doc, set_of_query)\n",
    "    return union_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processDocumentsFromFoler(path, mode_of_preprocessing):\n",
    "    \n",
    "    list_file_path = getFilePathList(path)\n",
    "\n",
    "    id_path_of_files = indexingDocument(list_file_path)\n",
    "\n",
    "    term_id = create_termID_forDocument(id_path_of_files, mode_of_preprocessing)\n",
    "\n",
    "    dictionary, vector_docs = createDictionaryAndVectorDoc(term_id)\n",
    "\n",
    "    calculateTF(dictionary)\n",
    "\n",
    "    calculateIDF(dictionary, len(id_path_of_files))\n",
    "\n",
    "    calculate_TF_IDF(dictionary)\n",
    "\n",
    "    normalizeDictionary(dictionary, vector_docs)\n",
    "    \n",
    "    return [dictionary, vector_docs, id_path_of_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processQuery(dictionary_of_docs, query, mode_of_preprocessing):\n",
    "    \n",
    "    term_id = create_termID_forQuery(dictionary_of_docs, query, mode_of_preprocessing)\n",
    "    \n",
    "    dictionary_of_query, vector_docs = createDictionaryAndVectorDoc(term_id)\n",
    "\n",
    "    calculateTF(dictionary_of_query)\n",
    "\n",
    "    calculateIDF_forQuery(dictionary_of_docs, dictionary_of_query)\n",
    "\n",
    "    calculate_TF_IDF(dictionary_of_query)\n",
    "\n",
    "    normalizeDictionary(dictionary_of_query, vector_docs)\n",
    "    \n",
    "    return dictionary_of_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateCosineSimilarity(dictionary_of_docs, dictionary_of_query):\n",
    "    \n",
    "    weight_of_documents = dict()\n",
    "    \n",
    "    for term in dictionary_of_query.keys():\n",
    "        weight_of_term_in_query = dictionary_of_query[term]['id_tf'][ID_OF_DOC_FOR_QUERY]      \n",
    "        \n",
    "        for id_of_doc in dictionary_of_docs[term]['id_tf'].keys():\n",
    "            \n",
    "            weight_of_term_in_document =  dictionary_of_docs[term]['id_tf'][id_of_doc]\n",
    "            \n",
    "            if id_of_doc not in weight_of_documents.keys():\n",
    "                weight_of_documents[id_of_doc] = weight_of_term_in_document * weight_of_term_in_query\n",
    "            else:\n",
    "                weight_of_documents[id_of_doc] += weight_of_term_in_document * weight_of_term_in_query\n",
    "          \n",
    "    \n",
    "    sorted_weight_of_documents = sorted(weight_of_documents.items(),\n",
    "                                        key = operator.itemgetter(1),\n",
    "                                        reverse = True)\n",
    "    return sorted_weight_of_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateEuclidSimilarity(dictionary_of_docs, dictionary_of_query, vector_docs):\n",
    "    \n",
    "    weight_of_documents = dict()\n",
    "    \n",
    "    set_of_query = set(dictionary_of_query.keys())\n",
    "    \n",
    "    for id_of_doc in vector_docs.keys():\n",
    "        union_term = Union(vector_docs[id_of_doc], set_of_query)\n",
    "        \n",
    "        for term in union_term:\n",
    "            \n",
    "            # weight_of_term_in_document\n",
    "            if term in vector_docs[id_of_doc]:\n",
    "                weight_of_term_in_document = dictionary_of_docs[term]['id_tf'][id_of_doc]\n",
    "            else:\n",
    "                weight_of_term_in_document = 0\n",
    "            \n",
    "            # weight_of_term_in_query\n",
    "            if term in dictionary_of_query.keys():\n",
    "                weight_of_term_in_query = dictionary_of_query[term]['id_tf'][ID_OF_DOC_FOR_QUERY]\n",
    "            else:\n",
    "                weight_of_term_in_query = 0\n",
    "                \n",
    "            if id_of_doc not in weight_of_documents.keys():\n",
    "                weight_of_documents[id_of_doc] = math.pow(weight_of_term_in_document - weight_of_term_in_query, 2)\n",
    "            else:\n",
    "                weight_of_documents[id_of_doc] += math.pow(weight_of_term_in_document - weight_of_term_in_query, 2)\n",
    "    \n",
    "    for id_of_doc in weight_of_documents.keys():\n",
    "        value = weight_of_documents[id_of_doc]\n",
    "        weight_of_documents[id_of_doc] = math.sqrt(value)\n",
    "    \n",
    "    sorted_weight_of_documents = sorted(weight_of_documents.items(),\n",
    "                                        key=operator.itemgetter(1),\n",
    "                                        reverse = False)\n",
    "    return sorted_weight_of_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_topK_result(sorted_weight_of_documents,\n",
    "                      id_path_of_files,\n",
    "                      top_k_results_to_return,\n",
    "                      similarity_measure):\n",
    "    \n",
    "    length_of_docs_returned = len(sorted_weight_of_documents)\n",
    "    \n",
    "    # Đối với code này:\n",
    "    # Euclid sẽ không bao giờ xảy ra trường hợp không có kết quả trả về, nhưng kết quả top-k có thể có những sqrt(2)\n",
    "    # Còn Cosine thì vẫn có trường hợp không có kết quả trả về, tại vì xét trên những term trong query\n",
    "    if length_of_docs_returned == 0:\n",
    "        print(\"\\nNo such song relate to query!!!\\n\")\n",
    "        return -1\n",
    "    \n",
    "    if similarity_measure == 'Cosine':\n",
    "        print(\"\\n--------------- Higher score is better ---------------\\n\")\n",
    "    elif similarity_measure == 'Euclid':\n",
    "        print(\"\\n--------------- Lower score is better ---------------\\n\")\n",
    "    \n",
    "    if top_k_results_to_return > length_of_docs_returned:\n",
    "        top_k_results_to_return = length_of_docs_returned\n",
    "        print(\"Only \" + str(top_k_results_to_return) + \" relate to query\\n\")\n",
    "        \n",
    "    for index in range(top_k_results_to_return):\n",
    "        score = sorted_weight_of_documents[index][1]\n",
    "        print('Top', index + 1, ':', score)\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "    for index in range(top_k_results_to_return):\n",
    "        file_id = sorted_weight_of_documents[index][0]\n",
    "        file_path = id_path_of_files[file_id]\n",
    "        score = sorted_weight_of_documents[index][1]\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read().strip().split('\\n')\n",
    "            file.close()\n",
    "            print('Position ' + str(index + 1) + ':')\n",
    "            print('Song:', content[0])\n",
    "            print('Artist:', content[1])\n",
    "            print('Lyric:', content[2])\n",
    "            print('\\n')      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_Vector_Space_Model(path, mode_of_preprocessing):\n",
    "    return processDocumentsFromFoler(path, mode_of_preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def searchDocumentWithQuery(dictionary_of_docs,\n",
    "                            vector_docs,\n",
    "                            id_path_of_files,\n",
    "                            query,\n",
    "                            top_k_results_to_return,\n",
    "                            mode_of_preprocessing,\n",
    "                            similarity_measure):\n",
    "    \n",
    "    dictionary_of_query = processQuery(dictionary_of_docs, query, mode_of_preprocessing)\n",
    "    \n",
    "    sorted_weight_of_documents = list()\n",
    "    \n",
    "    start_time = time.process_time()\n",
    "    \n",
    "    if similarity_measure == 'Cosine':\n",
    "        sorted_weight_of_documents = calculateCosineSimilarity(dictionary_of_docs, dictionary_of_query)\n",
    "    elif similarity_measure == 'Euclid':\n",
    "        sorted_weight_of_documents = calculateEuclidSimilarity(dictionary_of_docs, dictionary_of_query, vector_docs)\n",
    "        \n",
    "    end_time = time.process_time()\n",
    "    \n",
    "    print('\\nTime To Search: ', end_time - start_time, \" seconds\")\n",
    "        \n",
    "    print_topK_result(sorted_weight_of_documents, id_path_of_files, top_k_results_to_return, similarity_measure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Stopword removal options:\n",
      "\t 1. Stopword removal using NLTK stopword list\n",
      "\t 2. No stopword removal\n",
      "Please choose stopword removal option: 2\n",
      "\n",
      "* Word stemming options:\n",
      "\t 1. WordNet Lemmatizer\n",
      "\t 2. Porter Stemmer\n",
      "\t 3. No word stemming\n",
      "Please choose word stemming option: 3\n",
      "\n",
      "Method applied:\n",
      "\t - No stopword removal\n",
      "\t - No word stemming\n",
      "\t - Term weighting: TF-IDF\n",
      "\n",
      "\n",
      "--------------------------- Initializing Vector Space Model ---------------------------\n",
      "\n",
      "Initializing Finished\n",
      "Time To Build Model:  16.96875  seconds\n",
      "Ready To Search\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n* Stopword removal options:')\n",
    "print('\\t 1. Stopword removal using NLTK stopword list')\n",
    "print('\\t 2. No stopword removal')\n",
    "stopword_removal_option = int(input('Please choose stopword removal option: '))\n",
    "\n",
    "while(stopword_removal_option not in [1, 2]):\n",
    "    stopword_removal_option = int(input('Please choose stopword removal option again: '))\n",
    "    \n",
    "mode = str()\n",
    "if stopword_removal_option == 1:\n",
    "    mode += 'stopWord-'\n",
    "elif stopword_removal_option == 2:\n",
    "    mode += 'noStopWord-'\n",
    "    \n",
    "print('\\n* Word stemming options:')\n",
    "print('\\t 1. WordNet Lemmatizer')\n",
    "print('\\t 2. Porter Stemmer')\n",
    "print('\\t 3. No word stemming')\n",
    "word_stemming_option = int(input('Please choose word stemming option: '))\n",
    "\n",
    "while(word_stemming_option not in [1, 2, 3]):\n",
    "    word_stemming_option = int(input('Please choose word stemming option again: '))\n",
    "\n",
    "if word_stemming_option == 1:\n",
    "    mode += 'lemmatize'\n",
    "elif word_stemming_option == 2:\n",
    "    mode += 'stem'\n",
    "elif word_stemming_option == 3:\n",
    "    mode += 'noStem'\n",
    "  \n",
    "\n",
    "\n",
    "print('\\nMethod applied:')\n",
    "# Liệt kê các thông tin của phương pháp được áp dụng\n",
    "\n",
    "if stopword_removal_option == 1:   \n",
    "    print('\\t - Stopword removal: Stopword removal using NLTK stopword list')\n",
    "elif stopword_removal_option == 2:\n",
    "    print('\\t - No stopword removal')\n",
    "    \n",
    "if word_stemming_option == 1:\n",
    "    print('\\t - Word stemming: WordNet Lemmatizer')\n",
    "elif word_stemming_option == 2:\n",
    "    print('\\t - Word stemming: Porter Stemmer')\n",
    "elif word_stemming_option == 3:\n",
    "    print('\\t - No word stemming')\n",
    "\n",
    "print('\\t - Term weighting: TF-IDF')    \n",
    "\n",
    "    \n",
    "print('\\n\\n--------------------------- Initializing Vector Space Model ---------------------------\\n')\n",
    "\n",
    "file_path_of_docs = 'D:\\\\A_Truy_Van_Thong_Tin_Da_Phuong_Tien\\\\song_spotify\\\\Book3_txt'\n",
    "\n",
    "start_time = time.process_time()\n",
    "dict_of_docs, vector_of_docs, id_files = init_Vector_Space_Model(file_path_of_docs, mode)\n",
    "end_time = time.process_time()\n",
    "\n",
    "print('Initializing Finished')\n",
    "print('Time To Build Model: ', end_time - start_time, \" seconds\")\n",
    "print('Ready To Search')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for term in dict_of_docs.keys():\n",
    "#     if dict_of_docs[term]['ndoc'] == 0:\n",
    "#         print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(id_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Similarity measure options:\n",
      "\t 1. Cosine Similarity\n",
      "\t 2. Eulidean Distance\n",
      "Please choose Similarity Measure option: 1\n",
      "\t - Similarity Measure: Cosine Similarity\n",
      "\n",
      "Please type query to search: Take me down to the river bend ... Fly me up... on a silver wing\n",
      "\n",
      "Please choose top k results to return: 5\n",
      "\n",
      "Time To Search:  0.03125  seconds\n",
      "\n",
      "--------------- Higher score is better ---------------\n",
      "\n",
      "Top 1 : 0.36233035800842006\n",
      "Top 2 : 0.2169793319675935\n",
      "Top 3 : 0.1891265106511443\n",
      "Top 4 : 0.18774996831140786\n",
      "Top 5 : 0.1688559146205421\n",
      "\n",
      "\n",
      "Position 1:\n",
      "Song: CASTLE OF GLASS\n",
      "Artist: Linkin Park\n",
      "Lyric: Take me down to the river bend Take me down to the fighting end Wash the poison from off my skin Show me how to be whole again Fly me up on a silver wing Past the black where the sirens sing Warm me up in a nova's glow And drop me down to the dream below 'Cause I'm only a crack in this castle of glass Hardly anything there for you to see For you to see Bring me home in a blinding dream Through the secrets that I have seen Wash the sorrow from off my skin And show me how to be whole again 'Cause I'm only a crack in this castle of glass Hardly anything there, for you to see For you to see 'Cause I'm only a crack in this castle of glass Hardly anything else I need to be 'Cause I'm only a crack in this castle of glass Hardly anything there for you to see For you to see, for you to see\n",
      "\n",
      "\n",
      "Position 2:\n",
      "Song: Little Wing\n",
      "Artist: Jimi Hendrix\n",
      "Lyric: NA Well she's walking through the clouds With a circus mind that's running round Butterflies and zebras And moonbeams And fairytales That's all she ever thinks about Riding with the wind When I'm sad, she comes to me With a thousand smiles she gives to me free It's alright, she said, it's alright Take anything you want from me Anything,  anything Fly on, little wing Yeah, yeah, yeah, yeah, little wing\n",
      "\n",
      "\n",
      "Position 3:\n",
      "Song: Booty Loose (feat. Fly Boi Keno)\n",
      "Artist: Party Favor\n",
      "Lyric: Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, show me what this booty do Show me what this booty do Get loose with it Get loose with it You're too smooth with it You're to smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Get loose with it Get loose, loose  with it, with it Get loose with it Get loose,loose with it, with it Get loose with it Get loose,loose with it,with it Get loose with it Show me what this booty do Get loose with it Get loose with it You're too smooth with it You're too smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it\n",
      "\n",
      "\n",
      "Position 4:\n",
      "Song: Belong\n",
      "Artist: TENDER\n",
      "Lyric: And won't you stand in my And won't you stand in my way? Please stop me Won't you stand in my Won't you stand in my way? Please stop me No I don't belong, to anyone But I wish I did Then maybe I won't feel the shame Then maybe I could cherish my name Oh, I wish you'd stop me Before I go and hurt myself again Before I have to try to explain Mmm, what have I done? Take me under your wing Show me shelter from evil things Can't leave me alone Don't leave me alone for long No I don't belong, to anyone But I wish I did Then maybe I won't feel the shame Then maybe I could cherish my name Oh, I wish you'd stop me Before I go and hurt myself again Before I have to try to explain Mmm, what have I done? Take me under your wing Show me shelter from evil things Can't leave me alone Don't leave me alone for long\n",
      "\n",
      "\n",
      "Position 5:\n",
      "Song: Let Your Love Flow\n",
      "Artist: The Bellamy Brothers\n",
      "Lyric: NA There's a reason for the sun-shining sky And there's a reason why I'm feeling so high Must be the season when that Love light shines all around us So, let that feeling grab you deep inside And send you reeling where your love can't hide And then go stealing Through the moonlit nights with your lover Just let your love flow Like a mountain stream And let your love grow With the smallest of dreams And let your love show And you'll know what I mean It's the season Let your love fly Like a bird on a wing And let your love bind you To all living things And let your love shine And you'll know what I mean That's the reason There's a reason for the warm sweet nights And there's a reason for the candlelights Must be the season when those Love lights shine all around us So, let that wonder take you into space And lay you under its loving embrace Just feel the thunder as it warms your face You can't hold back Just let your love flow Like a mountain stream And let your love grow With the smallest of dreams And let your love show And you'll know what I mean It's the season Let your love fly Like a bird on a wing And let your love bind you To all living things And let your love shine And you'll know what I mean That's the reason Just let your love flow Like a mountain stream And let your love grow With the smallest of dreams And let your love show And you'll know what I mean It's the season Let your love fly Like a bird on a wing And let your love bind you To all living things And let your love shine And you'll know what I mean That's the reason Just let your love flow Like a mountain stream And let your love grow\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('\\n* Similarity measure options:')\n",
    "print('\\t 1. Cosine Similarity')\n",
    "print('\\t 2. Eulidean Distance')\n",
    "ranking_function_option = int(input('Please choose Similarity Measure option: '))\n",
    "simi_measure = str()\n",
    "\n",
    "while(ranking_function_option not in [1, 2]):\n",
    "    ranking_function_option = int(input('Please choose Similarity measure option again: '))\n",
    "\n",
    "if ranking_function_option == 1:\n",
    "    print('\\t - Similarity Measure: Cosine Similarity')\n",
    "    simi_measure = 'Cosine'\n",
    "elif ranking_function_option == 2:\n",
    "    print('\\t - Similarity Measuren: Eclidean Distance')\n",
    "    simi_measure = 'Euclid'\n",
    "\n",
    "\n",
    "query_to_search = input('\\nPlease type query to search: ')\n",
    "top_k = int(input('\\nPlease choose top k results to return: '))\n",
    "searchDocumentWithQuery(dict_of_docs, vector_of_docs, id_files, query_to_search, top_k, mode, simi_measure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Similarity measure options:\n",
      "\t 1. Cosine Similarity\n",
      "\t 2. Eulidean Distance\n",
      "Please choose Similarity Measure option: 2\n",
      "\t - Similarity Measuren: Eclidean Distance\n",
      "\n",
      "Please type query to search: Take me down to the river bend ... Fly me up... on a silver wing\n",
      "\n",
      "Please choose top k results to return: 5\n",
      "\n",
      "Time To Search:  2.859375  seconds\n",
      "\n",
      "--------------- Lower score is better ---------------\n",
      "\n",
      "Top 1 : 1.129309206543168\n",
      "Top 2 : 1.2514157327062867\n",
      "Top 3 : 1.273478299264542\n",
      "Top 4 : 1.2745587720372817\n",
      "Top 5 : 1.2892975493496122\n",
      "\n",
      "\n",
      "Position 1:\n",
      "Song: CASTLE OF GLASS\n",
      "Artist: Linkin Park\n",
      "Lyric: Take me down to the river bend Take me down to the fighting end Wash the poison from off my skin Show me how to be whole again Fly me up on a silver wing Past the black where the sirens sing Warm me up in a nova's glow And drop me down to the dream below 'Cause I'm only a crack in this castle of glass Hardly anything there for you to see For you to see Bring me home in a blinding dream Through the secrets that I have seen Wash the sorrow from off my skin And show me how to be whole again 'Cause I'm only a crack in this castle of glass Hardly anything there, for you to see For you to see 'Cause I'm only a crack in this castle of glass Hardly anything else I need to be 'Cause I'm only a crack in this castle of glass Hardly anything there for you to see For you to see, for you to see\n",
      "\n",
      "\n",
      "Position 2:\n",
      "Song: Little Wing\n",
      "Artist: Jimi Hendrix\n",
      "Lyric: NA Well she's walking through the clouds With a circus mind that's running round Butterflies and zebras And moonbeams And fairytales That's all she ever thinks about Riding with the wind When I'm sad, she comes to me With a thousand smiles she gives to me free It's alright, she said, it's alright Take anything you want from me Anything,  anything Fly on, little wing Yeah, yeah, yeah, yeah, little wing\n",
      "\n",
      "\n",
      "Position 3:\n",
      "Song: Booty Loose (feat. Fly Boi Keno)\n",
      "Artist: Party Favor\n",
      "Lyric: Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, show me what this booty do Show me what this booty do Get loose with it Get loose with it You're too smooth with it You're to smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Bend and make your booty loose, that booty loose Get loose with it Get loose, loose  with it, with it Get loose with it Get loose,loose with it, with it Get loose with it Get loose,loose with it,with it Get loose with it Show me what this booty do Get loose with it Get loose with it You're too smooth with it You're too smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it Get loose with it Get loose with it You're too smooth with it You're too smooth with it\n",
      "\n",
      "\n",
      "Position 4:\n",
      "Song: Belong\n",
      "Artist: TENDER\n",
      "Lyric: And won't you stand in my And won't you stand in my way? Please stop me Won't you stand in my Won't you stand in my way? Please stop me No I don't belong, to anyone But I wish I did Then maybe I won't feel the shame Then maybe I could cherish my name Oh, I wish you'd stop me Before I go and hurt myself again Before I have to try to explain Mmm, what have I done? Take me under your wing Show me shelter from evil things Can't leave me alone Don't leave me alone for long No I don't belong, to anyone But I wish I did Then maybe I won't feel the shame Then maybe I could cherish my name Oh, I wish you'd stop me Before I go and hurt myself again Before I have to try to explain Mmm, what have I done? Take me under your wing Show me shelter from evil things Can't leave me alone Don't leave me alone for long\n",
      "\n",
      "\n",
      "Position 5:\n",
      "Song: Let Your Love Flow\n",
      "Artist: The Bellamy Brothers\n",
      "Lyric: NA There's a reason for the sun-shining sky And there's a reason why I'm feeling so high Must be the season when that Love light shines all around us So, let that feeling grab you deep inside And send you reeling where your love can't hide And then go stealing Through the moonlit nights with your lover Just let your love flow Like a mountain stream And let your love grow With the smallest of dreams And let your love show And you'll know what I mean It's the season Let your love fly Like a bird on a wing And let your love bind you To all living things And let your love shine And you'll know what I mean That's the reason There's a reason for the warm sweet nights And there's a reason for the candlelights Must be the season when those Love lights shine all around us So, let that wonder take you into space And lay you under its loving embrace Just feel the thunder as it warms your face You can't hold back Just let your love flow Like a mountain stream And let your love grow With the smallest of dreams And let your love show And you'll know what I mean It's the season Let your love fly Like a bird on a wing And let your love bind you To all living things And let your love shine And you'll know what I mean That's the reason Just let your love flow Like a mountain stream And let your love grow With the smallest of dreams And let your love show And you'll know what I mean It's the season Let your love fly Like a bird on a wing And let your love bind you To all living things And let your love shine And you'll know what I mean That's the reason Just let your love flow Like a mountain stream And let your love grow\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n* Similarity measure options:')\n",
    "print('\\t 1. Cosine Similarity')\n",
    "print('\\t 2. Eulidean Distance')\n",
    "ranking_function_option = int(input('Please choose Similarity Measure option: '))\n",
    "simi_measure = str()\n",
    "\n",
    "while(ranking_function_option not in [1, 2]):\n",
    "    ranking_function_option = int(input('Please choose Similarity measure option again: '))\n",
    "\n",
    "if ranking_function_option == 1:\n",
    "    print('\\t - Similarity Measure: Cosine Similarity')\n",
    "    simi_measure = 'Cosine'\n",
    "elif ranking_function_option == 2:\n",
    "    print('\\t - Similarity Measuren: Eclidean Distance')\n",
    "    simi_measure = 'Euclid'\n",
    "\n",
    "\n",
    "query_to_search = input('\\nPlease type query to search: ')\n",
    "top_k = int(input('\\nPlease choose top k results to return: '))\n",
    "searchDocumentWithQuery(dict_of_docs, vector_of_docs, id_files, query_to_search, top_k, mode, simi_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haha = \"What have I done? And why? Who's this\"\n",
    "# haha = processQuery(dict_of_docs, haha, 'noStopWord-noStem')\n",
    "# haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_of_docs['he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordt = 'take'\n",
    "# if wordt in dict_of_docs.keys():\n",
    "#     print(\"true\")\n",
    "# else:\n",
    "#     print(\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index in range(5):\n",
    "#     id_of_doc = sort[index][0]\n",
    "#     print(vector_of_docs[id_of_doc], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"She makes the hairs on the back of my neck stand up Just one touch\"\n",
    "\n",
    "# term_id = create_termID_forQuery(dict_of_docs, q, mode)\n",
    "\n",
    "# dict_q, vector_querys = createDictionaryAndVectorDoc(term_id)\n",
    "\n",
    "# calculateTF(dict_q)\n",
    "\n",
    "# calculateIDF_forQuery(dict_of_docs, dict_q)\n",
    "\n",
    "# calculate_TF_IDF(dict_q)\n",
    "\n",
    "# normalizeDictionary(dict_q, vector_querys)\n",
    "    \n",
    "# dict_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vector_of_docs[sort[0][0]])\n",
    "# print(set(dict_q.keys()))\n",
    "# union_se = Union(vector_of_docs[sort[0][0]], set(dict_q.keys()))\n",
    "# print(union_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for term in union_se:\n",
    "#     print(term)\n",
    "#     print(\"Document - query: \", end = ' ')\n",
    "    \n",
    "#     index = 0\n",
    "#     id_doc = sort[index][0]\n",
    "    \n",
    "#     if term in vector_of_docs[id_doc]:\n",
    "#         print(dict_of_docs[term]['id_tf'][id_doc], end = ' - ')\n",
    "#     else:\n",
    "#         print(0, end = ' - ')\n",
    "#     if term in set(dict_q.keys()):\n",
    "#         print(dict_q[term]['id_tf'][ID_OF_DOC_FOR_QUERY], \"\\n\")\n",
    "#     else:\n",
    "#         print(0, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
